# ğŸŒ¬ï¸ Airflow ETL Pipeline - Remote Jobs

Pipeline de datos construido con **Apache Airflow** que extrae ofertas de trabajo remoto desde la API de [Remotive](https://remotive.com), las transforma, genera un archivo CSV limpio y visualiza los datos en un **Dashboard interactivo**.

![Python](https://img.shields.io/badge/Python-3.12-blue)
![Airflow](https://img.shields.io/badge/Apache%20Airflow-2.9.0-017CEE)
![Docker](https://img.shields.io/badge/Docker-Compose-2496ED)
![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-FF4B4B)

## ğŸ“ Estructura del Proyecto

```
Airflow_Proyect/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ helpers/
â”‚   â”‚   â”œâ”€â”€ extract.py          # ExtracciÃ³n desde API
â”‚   â”‚   â”œâ”€â”€ transform.py        # Limpieza de datos
â”‚   â”‚   â””â”€â”€ load.py             # Carga a CSV
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ raw/                # JSON descargados (generado)
â”‚   â”‚   â””â”€â”€ processed/          # CSV final (generado)
â”‚   â”œâ”€â”€ job_etl_dag.py          # DAG principal del ETL
â”‚   â””â”€â”€ primer_dag.py           # DAG de prueba
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ app.py                  # AplicaciÃ³n Streamlit
â”‚   â”œâ”€â”€ Dockerfile              # Imagen del dashboard
â”‚   â””â”€â”€ requirements.txt        # Dependencias del dashboard
â”œâ”€â”€ plugins/                    # Plugins personalizados de Airflow
â”œâ”€â”€ logs/                       # Logs de ejecuciÃ³n (generado)
â”œâ”€â”€ docker-compose.yaml         # ConfiguraciÃ³n de Docker
â”œâ”€â”€ requirements.txt            # Dependencias Python
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ CÃ³mo Ejecutar

### Prerrequisitos
- [Docker Desktop](https://www.docker.com/products/docker-desktop/)
- Git

### Pasos

1. **Clonar el repositorio**
   ```bash
   git clone https://github.com/tu-usuario/Airflow_Proyect.git
   cd Airflow_Proyect
   ```

2. **Levantar los contenedores**
   ```bash
   docker-compose up -d --build
   ```

3. **Esperar 2-3 minutos** para que Airflow inicialice la base de datos

4. **Acceder a las aplicaciones**

   | Servicio | URL | Credenciales |
   |----------|-----|--------------|
   | **Airflow UI** | http://localhost:8080 | admin / admin |
   | **Dashboard** | http://localhost:8501 | - |

5. **Ejecutar el ETL**
   - En Airflow, activar el toggle de `job_etl_pipeline`
   - Click en "Trigger DAG" â–¶ï¸
   - Esperar a que termine (cÃ­rculos verdes)
   - Ir al Dashboard para ver los resultados

### Detener el proyecto
```bash
docker-compose down
```

## ğŸ“Š DAGs Disponibles

| DAG | DescripciÃ³n | Schedule |
|-----|-------------|----------|
| `primer_dag_prueba` | DAG de prueba con un simple "Hola Mundo" | Diario |
| `job_etl_pipeline` | Pipeline ETL completo de trabajos remotos | Diario |

## ğŸ”„ Flujo del ETL

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Extract   â”‚ â”€â”€â–¶ â”‚  Transform   â”‚ â”€â”€â–¶ â”‚    Load     â”‚
â”‚  (API call) â”‚     â”‚  (Limpiar)   â”‚     â”‚   (CSV)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                   â”‚
        â–¼                   â–¼                   â–¼
   Remotive API      Filtrar campos      jobs.csv
   (JSON raw)        relevantes          (datos limpios)
```

1. **Extract**: Descarga trabajos remotos desde Remotive API
2. **Transform**: Limpia y estructura los datos (tÃ­tulo, empresa, categorÃ­a, salario, fecha)
3. **Load**: Genera archivo `jobs.csv` listo para anÃ¡lisis

## ğŸ“ˆ Dashboard

El dashboard de Streamlit incluye:

- ğŸ“Š **MÃ©tricas generales**: Total de trabajos, con salario, empresas Ãºnicas
- ğŸ“ **GrÃ¡fico de barras**: Trabajos por categorÃ­a
- ğŸ¢ **GrÃ¡fico circular**: Top 10 empresas contratando
- ğŸ’° **AnÃ¡lisis de salarios**: DistribuciÃ³n y comparaciÃ³n por categorÃ­a
- ğŸ“… **Timeline**: Publicaciones por fecha
- ğŸ” **Tabla interactiva**: BÃºsqueda y filtros en tiempo real

## ğŸ› ï¸ TecnologÃ­as

| TecnologÃ­a | VersiÃ³n | Uso |
|------------|---------|-----|
| Apache Airflow | 2.9.0 | OrquestaciÃ³n de workflows |
| PostgreSQL | 15 | Base de datos de metadatos |
| Streamlit | 1.29.0 | Dashboard interactivo |
| Plotly | 5.18.0 | GrÃ¡ficos interactivos |
| Docker | - | ContenedorizaciÃ³n |
| Python | 3.12 | Lenguaje principal |

## ğŸ“ Comandos Ãštiles

```bash
# Ver estado de contenedores
docker ps

# Ver logs de Airflow
docker logs airflow_webserver --tail 50

# Ver logs del scheduler
docker logs airflow_scheduler --tail 50

# Reiniciar un servicio
docker-compose restart airflow

# Reconstruir el dashboard
docker-compose up -d --build dashboard
```

## ğŸ—‚ï¸ Datos Generados

DespuÃ©s de ejecutar el ETL, encontrarÃ¡s:

- `dags/data/raw/` - Archivos JSON crudos de la API
- `dags/data/processed/jobs.csv` - Datos limpios en CSV

Ejemplo de datos:
```csv
title,company,category,salary,url,pub_date
Senior Data Engineer,TechCorp,Software Development,$80k-$120k,https://...,2025-01-15
```

## ğŸ‘©â€ğŸ’» Autora

**Camila**

---

â­ Si te sirviÃ³ este proyecto, Â¡dale una estrella!
